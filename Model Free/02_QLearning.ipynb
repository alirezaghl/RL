{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 243,
      "metadata": {
        "id": "slOTyQJMLK0G"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "min_epsilon = 0.01\n",
        "max_epsilon = 1.0\n",
        "decay_rate = 0.005\n",
        "num_episodes = 25000\n",
        "max_steps = 100\n",
        "alpha=0.8\n",
        "gamma=.7"
      ],
      "metadata": {
        "id": "qUJts54JAenq"
      },
      "execution_count": 1081,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QLearningAgent:\n",
        "  def __init__(self, env, alpha, gamma):\n",
        "    self.env = env\n",
        "    self.alpha = alpha\n",
        "    self.gamma = gamma\n",
        "    self.Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
        "\n",
        "\n",
        "\n",
        "  def epsilon_greedy(self, state, epsilon):\n",
        "    if np.random.uniform(0,1) < epsilon:\n",
        "      return env.action_space.sample()\n",
        "    else:\n",
        "      return np.argmax(self.Q[state][:])\n",
        "\n",
        "\n",
        "  def train(self, num_episodes):\n",
        "    rewards = []\n",
        "    epsilon = 1.0\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "      total_reward = 0\n",
        "      state = env.reset()\n",
        "      done = False\n",
        "\n",
        "      for i in range(max_steps):\n",
        "        action = self.epsilon_greedy(state, epsilon)\n",
        "        next_state, reward, done, _ = self.env.step(action)\n",
        "        self.Q[state][action] += alpha * (reward + gamma * np.max(self.Q[next_state][:]) - self.Q[state][action])\n",
        "\n",
        "        if done:\n",
        "          epsilon = min_epsilon + (max_epsilon-min_epsilon)*np.exp(-decay_rate*episode)\n",
        "          break\n",
        "        state = next_state\n",
        "\n",
        "\n",
        "\n",
        "    return self.Q\n",
        "\n",
        "  def evaluate(self, Q, num_episodes):\n",
        "    total_rewards = np.zeros(num_episodes)\n",
        "    for episode in range(num_episodes):\n",
        "      state = env.reset()\n",
        "      episode_reward = 0\n",
        "\n",
        "      for i in range(max_steps):\n",
        "        action = np.argmax(Q[state][:])\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        episode_reward += reward\n",
        "        state = next_state\n",
        "        if done:\n",
        "          total_rewards[episode] = episode_reward\n",
        "          break\n",
        "    return total_rewards\n",
        "\n",
        "\n",
        "\n",
        "env = gym.make('FrozenLake-v1')\n",
        "agent = QLearningAgent(env, alpha=0.8, gamma=.7)\n",
        "Q = agent.train(num_episodes)"
      ],
      "metadata": {
        "id": "TME97TqP6GPz"
      },
      "execution_count": 1077,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tot_reward = agent.evaluate(Q, 100)\n",
        "avg_reward = round(np.mean(tot_reward), 4)\n",
        "print(f\"Average reward: {avg_reward}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QY2ZpzbVEcGm",
        "outputId": "ade8c6ef-d014-4a54-ba6c-85cc1eec8aab"
      },
      "execution_count": 1080,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average reward: 0.7\n"
          ]
        }
      ]
    }
  ]
}